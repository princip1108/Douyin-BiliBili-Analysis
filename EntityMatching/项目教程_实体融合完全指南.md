# 实体融合项目完全教程

> 本教程将手把手教你从零开始完成一个基于TF-IDF和余弦相似度的实体匹配项目。

---

## 目录

1. [项目概述](#一项目概述)
2. [前置知识](#二前置知识)
3. [环境准备](#三环境准备)
4. [数据集介绍](#四数据集介绍)
5. [完整代码实现（分步骤）](#五完整代码实现分步骤)
6. [算法原理详解](#六算法原理详解)
7. [常见问题与调试](#七常见问题与调试)
8. [进阶优化方向](#八进阶优化方向)

---

## 一、项目概述

### 1.1 什么是实体融合？

**实体融合（Entity Resolution）** 也叫实体匹配、记录链接、数据去重。核心问题是：

> 给定两个数据集，找出哪些记录指的是同一个真实世界的实体。

**举例**：Amazon和Google都有商品数据，同一个商品在两个平台上的名称、描述可能不完全相同，我们需要找出这些匹配的商品对。

### 1.2 项目目标

1. 读取Amazon和Google两个商品数据集
2. 使用TF-IDF算法将文本转换为向量
3. 使用余弦相似度计算两个商品的相似程度
4. 通过阈值筛选，找出可能匹配的商品对
5. 评估匹配的精确率

### 1.3 最终结果

- 计算得到约 **244万对** 相似度
- 最大精确率约 **64.58%**
- 最佳阈值约 **0.81**

---

## 二、前置知识

### 2.1 必须掌握的Python知识

```python
# 1. 字典操作
dic = {}
dic['key'] = value
dic.get('key', default_value)

# 2. 列表推导式
result = [x for x in list if condition]

# 3. pandas基础
import pandas as pd
df = pd.read_csv('file.csv')
df.apply(function, axis=1)  # 对每一行应用函数

# 4. 正则表达式
import re
tokens = re.findall(r'\w+', text)  # 提取所有单词

# 5. lambda函数
df.apply(lambda x: some_function(x), axis=1)
```

### 2.2 需要理解的概念

| 概念 | 解释 |
|------|------|
| Token | 文本分词后的单个词 |
| 停用词 | 如"the", "is", "a"等无意义的高频词 |
| TF (词频) | 一个词在文档中出现的频率 |
| IDF (逆文档频率) | 衡量一个词的重要性，越稀有越重要 |
| TF-IDF | TF × IDF，综合衡量词的重要性 |
| 余弦相似度 | 两个向量夹角的余弦值，衡量相似程度 |
| 逆向索引 | 从词到文档的映射，加速搜索 |

---

## 三、环境准备

### 3.1 安装Python

1. 访问 https://www.python.org/downloads/
2. 下载Python 3.8+版本
3. 安装时勾选 "Add Python to PATH"

### 3.2 安装必要的库

打开命令行（cmd或PowerShell），执行：

```bash
pip install pandas matplotlib jupyter
```

### 3.3 启动Jupyter Notebook

```bash
# 进入项目目录
cd 你的项目路径

# 启动Jupyter
jupyter notebook
```

浏览器会自动打开，点击 "New" -> "Python 3" 创建新notebook。

### 3.4 项目文件结构

创建以下文件结构：

```
项目文件夹/
├── data/
│   ├── Amazon.csv          # Amazon商品数据
│   ├── Google.csv          # Google商品数据
│   ├── stopwords.txt       # 停用词表
│   └── Amazon_Google_perfectMapping.csv  # 标准答案
└── code.ipynb              # 你的代码
```

---

## 四、数据集介绍

### 4.1 Amazon.csv 和 Google.csv

每个CSV文件包含以下字段：

| 字段 | 说明 | 示例 |
|------|------|------|
| id | 商品唯一ID | "b000jz4hqo" |
| title | 商品标题 | "Linksys Wireless Router" |
| description | 商品描述 | "High-speed wireless..." |
| manufacturer | 制造商 | "Linksys" |
| price | 价格 | "49.99" |

### 4.2 stopwords.txt

包含约127个英文停用词，每行一个：

```
a
about
above
after
again
...
```

### 4.3 Amazon_Google_perfectMapping.csv

标准答案文件，包含真正匹配的商品对：

| idAmazon | idGoogleBase |
|----------|--------------|
| b000jz4hqo | http://google.com/base/123 |
| ... | ... |

---

## 五、完整代码实现（分步骤）

### 步骤1：导入库和读取数据

```python
# Cell 1: 导入库和读取数据
import pandas as pd
import re
import math
from collections import Counter

# 设置数据路径
DATA_PATH = "./data"

# 读取停用词
with open(DATA_PATH + "/stopwords.txt", 'r') as file:
    stopwords = file.read().split('\n')

# 读取商品数据
amazon_df = pd.read_csv(DATA_PATH + "/Amazon.csv")
google_df = pd.read_csv(DATA_PATH + "/Google.csv")

# 查看数据
print(f"Amazon数据: {len(amazon_df)} 条")
print(f"Google数据: {len(google_df)} 条")
print("\nAmazon数据示例:")
print(amazon_df.head(2))
```

**运行结果应该是：**
```
Amazon数据: 1363 条
Google数据: 3226 条
```

---

### 步骤2：读取标准答案

```python
# Cell 2: 读取标准答案（perfectMapping）
perfectMap_df = pd.read_csv(DATA_PATH + "/Amazon_Google_perfectMapping.csv")

# 将标准答案转换为元组列表，方便后续查找
perfectMap = []

def buildPerfectMap(row):
    perfectMap.append((row['idAmazon'], row['idGoogleBase']))

perfectMap_df.apply(buildPerfectMap, axis=1)

print(f"标准答案共 {len(perfectMap)} 对匹配")
print(f"示例: {perfectMap[0]}")
```

**解释：**
- `perfectMap` 存储了所有真正匹配的(Amazon商品ID, Google商品ID)对
- 这是我们的"标准答案"，用于评估我们算法的准确性

---

### 步骤3：实现分词函数

```python
# Cell 3: 分词函数

# 正则表达式：匹配连续的字母数字
split_regex = r'\w+'

def simple_tokenize(string):
    """
    简单分词：使用正则表达式分割文本，转换为小写
    
    参数:
        string: 输入文本
    返回:
        tokens列表
    
    示例:
        "Hello World!" -> ["hello", "world"]
    """
    # 处理空值
    if pd.isna(string) or string is None:
        return []
    # 使用正则提取所有单词，并转小写
    return re.findall(split_regex, string.lower())


def tokenize(string):
    """
    分词并去除停用词
    
    参数:
        string: 输入文本
    返回:
        去除停用词后的tokens列表
    
    示例:
        "This is a test" -> ["test"]  # "this", "is", "a" 是停用词
    """
    tokens = simple_tokenize(string)
    # 过滤掉停用词
    return [t for t in tokens if t not in stopwords]


# 测试
test_text = "This is a great product for testing"
print(f"原始文本: {test_text}")
print(f"simple_tokenize: {simple_tokenize(test_text)}")
print(f"tokenize (去除停用词): {tokenize(test_text)}")
```

**运行结果：**
```
原始文本: This is a great product for testing
simple_tokenize: ['this', 'is', 'a', 'great', 'product', 'for', 'testing']
tokenize (去除停用词): ['great', 'product', 'testing']
```

---

### 步骤4：将记录转换为tokens

```python
# Cell 4: 将商品记录转换为tokens

def rec2tok(row, dic):
    """
    将一条商品记录转换为tokens，存入字典
    
    参数:
        row: DataFrame的一行
        dic: 存储结果的字典 {id: [tokens]}
    
    处理流程:
        1. 获取商品ID
        2. 合并title, description, manufacturer文本
        3. 分词并去除停用词
        4. 存入字典
    """
    record_id = row['id']
    
    # 收集所有非空文本字段
    text_parts = []
    
    if pd.notna(row.get('title')):
        text_parts.append(str(row['title']))
    
    if pd.notna(row.get('description')):
        text_parts.append(str(row['description']))
    
    if pd.notna(row.get('manufacturer')):
        text_parts.append(str(row['manufacturer']))
    
    # 合并所有文本
    combined_text = ' '.join(text_parts)
    
    # 分词并存入字典
    dic[record_id] = tokenize(combined_text)


# 创建两个字典存储分词结果
amazon_rec2tok = {}
google_rec2tok = {}

# 对每条记录应用rec2tok函数
amazon_df.apply(lambda x: rec2tok(x, amazon_rec2tok), axis=1)
google_df.apply(lambda x: rec2tok(x, google_rec2tok), axis=1)

# 验证
print(f"Amazon记录数: {len(amazon_rec2tok)}")
print(f"Google记录数: {len(google_rec2tok)}")

# 查看一个示例
sample_id = list(amazon_rec2tok.keys())[0]
print(f"\n示例ID: {sample_id}")
print(f"Tokens: {amazon_rec2tok[sample_id][:10]}...")  # 只显示前10个
```

---

### 步骤5：实现TF计算

```python
# Cell 5: TF (词频) 计算

def inc(key, dic):
    """
    辅助函数：在字典中对某个键的值加1
    如果键不存在，初始化为1
    """
    if key in dic:
        dic[key] += 1
    else:
        dic[key] = 1


def tf(tokens):
    """
    计算词频 TF (Term Frequency)
    
    公式: TF(token) = 该token出现次数 / 总token数
    
    参数:
        tokens: 分词列表
    返回:
        字典 {token: tf_value}
    
    示例:
        ["apple", "banana", "apple"] 
        -> {"apple": 2/3, "banana": 1/3}
    """
    if not tokens:
        return {}
    
    tf_dict = {}
    total = len(tokens)
    
    # 统计每个token的出现次数
    for token in tokens:
        inc(token, tf_dict)
    
    # 除以总数得到频率
    for token in tf_dict:
        tf_dict[token] = tf_dict[token] / total
    
    return tf_dict


# 测试
test_tokens = ["apple", "banana", "apple", "cherry"]
print(f"输入tokens: {test_tokens}")
print(f"TF结果: {tf(test_tokens)}")
```

**运行结果：**
```
输入tokens: ['apple', 'banana', 'apple', 'cherry']
TF结果: {'apple': 0.5, 'banana': 0.25, 'cherry': 0.25}
```

---

### 步骤6：实现IDF计算

```python
# Cell 6: IDF (逆文档频率) 计算

def idf(rec2tok):
    """
    计算逆文档频率 IDF (Inverse Document Frequency)
    
    公式: IDF(token) = 文档总数 / 包含该token的文档数
    
    参数:
        rec2tok: 字典 {record_id: [tokens]}
    返回:
        字典 {token: idf_value}
    
    原理:
        - 如果一个词在很多文档中都出现，IDF就低（不重要）
        - 如果一个词只在少数文档中出现，IDF就高（重要）
    """
    num_docs = len(rec2tok)  # 文档总数
    doc_freq = {}  # 每个token在多少文档中出现
    
    # 统计每个token出现在多少个文档中
    for record_id, tokens in rec2tok.items():
        # 使用set去重，每个文档中的token只计一次
        unique_tokens = set(tokens)
        for token in unique_tokens:
            inc(token, doc_freq)
    
    # 计算IDF
    idf_dict = {}
    for token, df in doc_freq.items():
        idf_dict[token] = num_docs / df
    
    return idf_dict


# 测试：简单示例
test_rec2tok = {
    'doc1': ['apple', 'banana'],
    'doc2': ['apple', 'cherry'],
    'doc3': ['apple', 'date'],
}
test_idf = idf(test_rec2tok)
print(f"测试IDF结果:")
print(f"  apple出现在3个文档中, IDF = 3/3 = {test_idf['apple']}")
print(f"  banana出现在1个文档中, IDF = 3/1 = {test_idf['banana']}")
```

**运行结果：**
```
测试IDF结果:
  apple出现在3个文档中, IDF = 3/3 = 1.0
  banana出现在1个文档中, IDF = 3/1 = 3.0
```

---

### 步骤7：实现TF-IDF计算

```python
# Cell 7: TF-IDF 计算

def tfidf(tokens, idfs):
    """
    计算 TF-IDF 值
    
    公式: TF-IDF(token) = TF(token) × IDF(token)
    
    参数:
        tokens: 分词列表
        idfs: 全局IDF字典
    返回:
        字典 {token: tfidf_value}
    
    原理:
        TF-IDF综合考虑了：
        - 词在当前文档中的重要性（TF）
        - 词在整个语料库中的稀有程度（IDF）
    """
    tf_dict = tf(tokens)
    tfidf_dict = {}
    
    for token, tf_val in tf_dict.items():
        # 获取IDF值，如果token不在IDF字典中，默认为0
        idf_val = idfs.get(token, 0)
        tfidf_dict[token] = tf_val * idf_val
    
    return tfidf_dict


# 计算全局IDF（合并两个数据集）
# 使用Counter合并两个IDF字典
idfs_full = dict(Counter(idf(amazon_rec2tok)) + Counter(idf(google_rec2tok)))

print(f"全局IDF词汇量: {len(idfs_full)}")
```

---

### 步骤8：构建逆向索引

```python
# Cell 8: 构建逆向索引

def invertIndex(forward_index):
    """
    构建逆向索引
    
    从正向索引 {id: [tokens]} 
    转换为逆向索引 {token: [ids]}
    
    参数:
        forward_index: 正向索引字典
    返回:
        逆向索引字典
    
    作用:
        快速找出包含某个token的所有文档
        避免暴力比较所有文档对（O(n²) -> 更优）
    """
    inverted = {}
    
    for record_id, tokens in forward_index.items():
        # 去重，避免重复添加
        unique_tokens = set(tokens)
        for token in unique_tokens:
            if token not in inverted:
                inverted[token] = []
            inverted[token].append(record_id)
    
    return inverted


# 构建Amazon的逆向索引
amazon_inv = invertIndex(amazon_rec2tok)

print(f"逆向索引词汇量: {len(amazon_inv)}")

# 示例：查看某个token对应哪些商品
sample_token = list(amazon_inv.keys())[0]
print(f"\n示例: token '{sample_token}' 出现在以下商品中:")
print(f"  {amazon_inv[sample_token][:5]}...")  # 只显示前5个
```

---

### 步骤9：实现点积和模长计算

```python
# Cell 9: 向量点积和模长计算

def dotprod(a, b):
    """
    计算两个向量的点积
    
    只计算两个向量共有的维度（token）
    
    参数:
        a, b: 两个向量，都是字典 {token: weight}
    返回:
        点积结果（浮点数）
    
    公式: a · b = Σ(a[i] × b[i])
    """
    result = 0.0
    for token, value in a.items():
        if token in b:
            result += value * b[token]
    return result


def norm(a):
    """
    计算向量的模长（欧几里得范数）
    
    参数:
        a: 向量，字典 {token: weight}
    返回:
        模长（浮点数）
    
    公式: |a| = √(Σ(a[i]²))
    """
    if not a:
        return 0.0
    return math.sqrt(sum(v ** 2 for v in a.values()))


# 测试
vec_a = {'x': 3, 'y': 4}
vec_b = {'x': 1, 'y': 1, 'z': 5}  # z不在a中，不参与计算

print(f"向量a: {vec_a}")
print(f"向量b: {vec_b}")
print(f"点积 a·b = 3*1 + 4*1 = {dotprod(vec_a, vec_b)}")
print(f"|a| = √(3² + 4²) = √25 = {norm(vec_a)}")
```

**运行结果：**
```
向量a: {'x': 3, 'y': 4}
向量b: {'x': 1, 'y': 1, 'z': 5}
点积 a·b = 3*1 + 4*1 = 7.0
|a| = √(3² + 4²) = √25 = 5.0
```

---

### 步骤10：预计算所有TF-IDF向量和模长

```python
# Cell 10: 预计算所有商品的TF-IDF向量和模长

# 计算每个商品的TF-IDF向量
# google_weights[id] = {token: tfidf_value}
google_weights = {
    record_id: tfidf(google_rec2tok[record_id], idfs_full) 
    for record_id in google_rec2tok
}

amazon_weights = {
    record_id: tfidf(amazon_rec2tok[record_id], idfs_full) 
    for record_id in amazon_rec2tok
}

# 预计算每个商品向量的模长
# 这样在计算余弦相似度时就不需要重复计算
google_norm = {record_id: norm(google_weights[record_id]) for record_id in google_weights}
amazon_norm = {record_id: norm(amazon_weights[record_id]) for record_id in amazon_weights}

print(f"Google商品TF-IDF向量: {len(google_weights)} 个")
print(f"Amazon商品TF-IDF向量: {len(amazon_weights)} 个")
```

---

### 步骤11：计算相似度

```python
# Cell 11: 计算所有相似度

def buildSim(google_id, google_weight, google_norm_val, 
             amazon_weights, amazon_norms, amazon_inv, sims):
    """
    为一个Google商品计算与所有可能匹配的Amazon商品的相似度
    
    参数:
        google_id: Google商品ID
        google_weight: Google商品的TF-IDF向量
        google_norm_val: Google商品向量的模长
        amazon_weights: 所有Amazon商品的TF-IDF向量
        amazon_norms: 所有Amazon商品向量的模长
        amazon_inv: Amazon的逆向索引
        sims: 存储相似度结果的字典
    
    核心优化:
        不是比较所有Google-Amazon商品对（O(n×m)太慢）
        而是只比较有共同token的商品对
    """
    # 遍历Google商品中的每个token
    for token in google_weight:
        # 如果这个token在Amazon逆向索引中存在
        if token in amazon_inv:
            # 获取所有包含这个token的Amazon商品
            for amazon_id in amazon_inv[token]:
                # 避免重复计算
                if (amazon_id, google_id) not in sims:
                    # 计算余弦相似度
                    # cos(θ) = (a·b) / (|a| × |b|)
                    similarity = dotprod(google_weight, amazon_weights[amazon_id]) \
                                 / google_norm_val / amazon_norms[amazon_id]
                    sims[(amazon_id, google_id)] = similarity


# 存储所有相似度
# 键: (amazon_id, google_id)
# 值: 余弦相似度
sims = {}

# 遍历每个Google商品
print("开始计算相似度...")
for i, google_id in enumerate(google_weights):
    buildSim(
        google_id,
        google_weights[google_id],
        google_norm[google_id],
        amazon_weights,
        amazon_norm,
        amazon_inv,
        sims
    )
    # 每处理500个打印进度
    if (i + 1) % 500 == 0:
        print(f"已处理 {i + 1}/{len(google_weights)} 个Google商品")

print(f"\n计算完成！共 {len(sims)} 对相似度")
```

**运行结果：**
```
开始计算相似度...
已处理 500/3226 个Google商品
已处理 1000/3226 个Google商品
...
计算完成！共 2441100 对相似度
```

---

### 步骤12：评估精确率

```python
# Cell 12: 精确率评估

true_dup_sims = []  # 存储超过阈值的匹配对

def truepos(threshold):
    """
    找出所有相似度超过阈值的匹配对
    """
    global true_dup_sims
    true_dup_sims = []
    for pair in sims:
        if sims[pair] > threshold:
            true_dup_sims.append(pair)


def falsepos(threshold):
    """
    计算假阳性数量（我们认为匹配但实际不匹配的数量）
    """
    count = 0
    for pair in true_dup_sims:
        if pair not in perfectMap:
            count += 1
    return count


def precision(threshold):
    """
    计算精确率
    
    精确率 = 真阳性 / (真阳性 + 假阳性)
           = 真正匹配对数 / 我们预测的匹配对数
    
    参数:
        threshold: 相似度阈值
    返回:
        精确率（0到1之间）
    """
    truepos(threshold)  # 先筛选出超过阈值的匹配对
    
    if len(true_dup_sims) == 0:
        return 0.0
    
    true_positive = len(true_dup_sims) - falsepos(threshold)
    return true_positive / len(true_dup_sims)


# 测试一个阈值
test_threshold = 0.5
print(f"阈值 {test_threshold} 的精确率: {precision(test_threshold):.4f}")
```

---

### 步骤13：绘制精确率曲线并找最优阈值

```python
# Cell 13: 绘制精确率曲线

# 使用matplotlib绘图
%matplotlib inline
import matplotlib.pyplot as plt

# 定义阈值范围：0.02 到 0.99
nthresholds = 100
thresholds = [float(n) / nthresholds for n in range(2, nthresholds)]

# 计算每个阈值对应的精确率
print("计算各阈值的精确率...")
precisions = []
for t in thresholds:
    p = precision(t)
    precisions.append(p)

# 绘制曲线
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, 'b-', linewidth=2)
plt.xlabel('阈值 (Threshold)', fontsize=12)
plt.ylabel('精确率 (Precision)', fontsize=12)
plt.title('精确率-阈值关系曲线', fontsize=14)
plt.grid(True, alpha=0.3)
plt.show()

# 找出最大精确率和对应阈值
max_precision = max(precisions)
best_threshold = thresholds[precisions.index(max_precision)]

print(f"\n===== 最终结果 =====")
print(f"最大精确率: {max_precision:.4f}")
print(f"最佳阈值: {best_threshold}")
```

**运行结果：**
```
===== 最终结果 =====
最大精确率: 0.6458
最佳阈值: 0.81
```

---

## 六、算法原理详解

### 6.1 为什么用TF-IDF？

**问题**：如何判断两段文本是否相似？

**朴素方法**：比较共同词的数量
- 缺点：高频词（如"the", "product"）会主导结果

**TF-IDF解决方案**：
- **TF**：一个词在文档中出现得多，说明这个词对这个文档重要
- **IDF**：一个词在整个语料库中很少出现，说明这个词有区分度
- **TF-IDF = TF × IDF**：既在当前文档中出现多，又在整体语料中稀有的词最重要

### 6.2 余弦相似度图解

```
                   向量b
                   ↗
                 /
               /  θ（夹角）
             /
           /
    ——————————————→ 向量a

cos(θ) = 1.0  → 完全相同方向（完全匹配）
cos(θ) = 0.5  → 有一定相似性
cos(θ) = 0.0  → 正交（完全不相关）
```

**公式**：
```
cos(θ) = (a · b) / (|a| × |b|)

其中：
- a · b = Σ(aᵢ × bᵢ)  点积
- |a| = √(Σaᵢ²)       模长
```

### 6.3 为什么用逆向索引？

**暴力方法**：
- 比较所有Amazon商品 × 所有Google商品
- 1363 × 3226 = 4,396,138 次比较
- 每次比较需要计算完整的余弦相似度

**逆向索引优化**：
- 只比较有共同token的商品对
- 如果两个商品没有任何共同词，它们的相似度必然为0
- 实际只需要计算约244万对，且计算更快

---

## 七、常见问题与调试

### 7.1 运行太慢怎么办？

**解决方案1**：使用小数据集测试

```python
# 使用小数据集
amazon_df = pd.read_csv(DATA_PATH + "/Amazon_small.csv")
google_df = pd.read_csv(DATA_PATH + "/Google_small.csv")
```

**解决方案2**：添加进度显示

```python
from tqdm import tqdm  # 需要pip install tqdm

for google_id in tqdm(google_weights):
    buildSim(...)
```

### 7.2 结果和标准答案不一样？

可能原因：
1. **IDF计算方式不同**：可以尝试分别计算两个数据集的IDF
2. **浮点精度**：正常现象
3. **阈值步长**：可以用更细的步长（如0.001）

### 7.3 内存不足？

**解决方案**：分批处理

```python
batch_size = 500
for i in range(0, len(google_weights), batch_size):
    batch_ids = list(google_weights.keys())[i:i+batch_size]
    for google_id in batch_ids:
        buildSim(...)
    # 可选：保存中间结果
```

### 7.4 如何验证代码正确性？

**单元测试示例**：

```python
# 测试TF
assert tf(['a', 'a', 'b']) == {'a': 2/3, 'b': 1/3}

# 测试点积
assert dotprod({'x': 1}, {'x': 2}) == 2.0

# 测试模长
assert norm({'x': 3, 'y': 4}) == 5.0  # 3-4-5三角形
```

---

## 八、进阶优化方向

### 8.1 使用log-IDF

```python
import math

def idf_log(rec2tok):
    num_docs = len(rec2tok)
    doc_freq = {}
    for record_id, tokens in rec2tok.items():
        for token in set(tokens):
            inc(token, doc_freq)
    
    idf_dict = {}
    for token, df in doc_freq.items():
        # 使用log避免IDF值过大
        idf_dict[token] = math.log(num_docs / df)
    return idf_dict
```

### 8.2 加入价格特征

```python
def price_similarity(price1, price2):
    if pd.isna(price1) or pd.isna(price2):
        return 0.5  # 缺失时给中间值
    
    # 归一化价格差异
    diff = abs(price1 - price2)
    max_price = max(price1, price2)
    return 1 - (diff / max_price)

# 综合相似度 = 0.8 * 文本相似度 + 0.2 * 价格相似度
```

### 8.3 使用更好的分词

```python
# 安装: pip install nltk
import nltk
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

def tokenize_advanced(string):
    tokens = simple_tokenize(string)
    # 词干提取：running, runs, ran -> run
    tokens = [stemmer.stem(t) for t in tokens]
    return [t for t in tokens if t not in stopwords]
```

### 8.4 使用机器学习

当有标注数据时，可以训练分类模型：

```python
# 特征：文本相似度、价格差异、制造商匹配等
# 标签：是否匹配（0或1）
# 模型：逻辑回归、随机森林、XGBoost等

from sklearn.ensemble import RandomForestClassifier

features = [...]  # 提取特征
labels = [...]    # 0或1

model = RandomForestClassifier()
model.fit(features, labels)
```

---

## 附录：完整代码一览

将以上所有步骤的代码按顺序放入Jupyter Notebook的各个Cell中即可运行。

**关键变量速查表**：

| 变量名 | 类型 | 说明 |
|--------|------|------|
| `amazon_df` | DataFrame | Amazon原始数据 |
| `google_df` | DataFrame | Google原始数据 |
| `stopwords` | List | 停用词列表 |
| `perfectMap` | List | 标准答案匹配对 |
| `amazon_rec2tok` | Dict | {id: [tokens]} |
| `google_rec2tok` | Dict | {id: [tokens]} |
| `amazon_inv` | Dict | {token: [ids]} 逆向索引 |
| `idfs_full` | Dict | {token: idf} 全局IDF |
| `amazon_weights` | Dict | {id: {token: tfidf}} |
| `google_weights` | Dict | {id: {token: tfidf}} |
| `amazon_norm` | Dict | {id: norm} 模长 |
| `google_norm` | Dict | {id: norm} 模长 |
| `sims` | Dict | {(amazon_id, google_id): similarity} |

---

## 总结

恭喜你完成了整个实体融合项目！通过这个项目，你学会了：

1. ✅ 文本分词和停用词过滤
2. ✅ TF-IDF向量化文本
3. ✅ 余弦相似度计算
4. ✅ 逆向索引优化搜索
5. ✅ 精确率评估方法

这些技术广泛应用于：
- 搜索引擎
- 推荐系统
- 数据清洗
- 知识图谱构建

---

*文档编写日期：2024年12月*
*祝学习顺利！*
